# Machine Learning

In this repo I explored the algorithms and structures that are used to create LLM's.

The following is what I worked on first to last

*  Building Neural Network and Layer classes
*  Linear Regression
*  Multivariate Linear Regression
*  Nerual Network using SoftMax
*  Positional Encoding
*  Transformer
*  Word2Vec

The end goal was to learn about and impliment my own transformer model, I was able to impliment a decoder only architeture.

Along with using my own Word2Vec to process text to get word embeddings I was able to successfully have the LLM train and predict.

It was very interesting looking through papers and youtube videos to learn about the math behind LLM's and its suprisingly simple.

Testing my model its able to generate text but due to my limited computational resources I am not able to get very complex/predictable outputs.
